---
title: "EXERCISE 8.2.3"
author: "Jahedur Rahman"
date: "2/3/2022"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)
```
#i  
Explain any transformations or modifications you made to the dataset

```{r, echo=FALSE, include=FALSE}
#load libraries
library(readxl)
library(dplyr)
library(ggplot2)
library(psych)
library(GGally)
library(purrr)
library(QuantPsyc)
library(car)
```

```{r}
#load housing df
housing_dataset_df <- read_xlsx("C:/Users/jahed/OneDrive/Documents/GitHub/dsc520/data/week-7-housing.xlsx")
#Data changes
colnames(housing_dataset_df)[c(1,2)]<-c("sale_date", "sale_price")
colnames(housing_dataset_df)
```

#ii  
Create two variables; one that will contain the variables Sale Price and Square Foot of Lot (same variables used from previous assignment on simple regression) and one that will contain Sale Price and several additional predictors of your choice. Explain the basis for your additional predictor selections.

```{r echo=TRUE, include=TRUE}
saleprice_sqftlot <- (housing_dataset_df)[c(2,22)]
head(saleprice_sqftlot)
saleprice_predictors <- (housing_dataset_df)[c(2,13,14,15,16,22)]
head(saleprice_predictors)
```

#iii  
Execute a summary() function on two variables defined in the previous step to compare the model results. What are the R2 and Adjusted R2 statistics? Explain what these results tell you about the overall model. Did the inclusion of the additional predictors help explain any large variations found in Sale Price?

```{r echo=TRUE, include=TRUE}
housing_lm1 <- lm(sale_price ~ sq_ft_lot, data = saleprice_sqftlot)
housing_lm2 <- lm(sale_price ~ building_grade 
                   + square_feet_total_living + bedrooms 
                   + bath_full_count + sq_ft_lot, data = saleprice_predictors)
summary(housing_lm1)
summary(housing_lm2)
```
> The first model, "housing_lm1", has an R2 statistic which is the square of the correlation of the sale price and the lot size. This is a single variable regression. This shows us the variability in sale price that is affected by the lot size. The adjusted R2 statistics has the same measure. The difference is that it gives us information on how it could change if it was given more data. Since the values of the R2 statistic and the adjusted R2 statistic are close, there is good generalizability. The second model, "housing_lm2", has an R2 statistic and average R2 statistic that are higher than the first model. These results tell us there is about 21% variability in the sale price. Just like in the first model the R2 and adjusted R2 values are very close so there is good generalizability. 

#iv  
Considering the parameters of the multiple regression model you have created. What are the standardized betas for each parameter and what do the values indicate?
```{r}
lm.beta(housing_lm1)
lm.beta(housing_lm2)
```
> The standardized betas for the linear model "housing_lm1" indicates the sale price increases by 0.11 standard deviations when there is an increase in standard deviations for the property's lot size by square feet. The standardized betas for the linear model "housing_lm2" indicates that the sale price is affected by each variable in a similar way. Square feet of total living space has the most predictive impact compared to all the variables.

#v  
Calculate the confidence intervals for the parameters in your model and explain what the results indicate.
```{r}
confint(housing_lm1)
confint(housing_lm2)
```
> The confidence intervals calculated for "housing_lm1" have a small range. This indicates that the predictor's _b_ value is close to the real _b_ value. The confidence intervals calculated for "housing_lm2" have a larger range. In addition, these values cross zero and include negative values. This indicates that the sale price can increase or decrease depending on the number of bedrooms. This makes the output for sale price not consistent. However, the other variables do have better consistency and shorter range.

#vi  
Assess the improvement of the new model compared to your original model (simple regression model) by testing whether this change is significant by performing an analysis of variance.
```{r}
anova(housing_lm1, housing_lm2)
```
>The analysis shows that the second model, "housing_lm2", has a p value below 0.001 so it did perform better with 4 degrees of freedom.

#vii  
Perform casewise diagnostics to identify outliers and/or influential cases, storing each function's output in a dataframe assigned to a unique variable name.

```{r}
saleprice_predictors$standardized.residuals<-rstandard(housing_lm2)
saleprice_predictors$studentized.residuals<-rstudent(housing_lm2)
saleprice_predictors$cooks.distance<-cooks.distance(housing_lm2)
saleprice_predictors$dfbeta<-dfbeta(housing_lm2)
saleprice_predictors$dffit<-dffits(housing_lm2)
saleprice_predictors$leverage<-hatvalues(housing_lm2)
saleprice_predictors$covariance.ratios<-covratio(housing_lm2)
```

#viii  
Calculate the standardized residuals using the appropriate command, specifying those that are +-2, storing the results of large residuals in a variable you create.
```{r}
saleprice_predictors$large.residual<-
  saleprice_predictors$standardized.residuals > 2 | saleprice_predictors$standardized.residuals < -2
```

#ix  
Use the appropriate function to show the sum of large residuals.
```{r}
sum(saleprice_predictors$large.residual)
```

#x  
Which specific variables have large residuals (only cases that evaluate as TRUE)?
```{r}
large_res<-saleprice_predictors%>%
  filter(large.residual == 1)
```

#xi  
Investigate further by calculating the leverage, cooks distance, and covariance ratios. Comment on all cases that are problematic.

```{r}
#Percentage of sample with residuals over (+/-)2
nrow(housing_dataset_df)
nrow(large_res)
322/12865*100
#calculate average leverage for comparison with 4 parameters
avg_leverage = (4+1)/12865
avg_leverage
#calculate limit(s) leverage should not exceed
leverage_limit= avg_leverage*2
leverage_limit
leverage_limit3 = avg_leverage*3
leverage_limit3
#get count of samples over leverage limit
large_res%>%
  filter(leverage > leverage_limit)%>%
  nrow()
large_res%>%
  filter(leverage > leverage_limit3)%>%
  nrow()
```
> After calculating the average leverage, I doubled it to create a boundary. 111 cases from the large residuals dataframe go passed this boundary. After tripling the average leverage and applying it to the large residuals dataframe, 87 cases are beyond it. Compared to other cases, these unusually large residuals are not appropriate to our model's outcome.

```{r}  
#Search for cook's distance values that exceed 1.0 
large_res%>%
  filter(cooks.distance > 1)%>%
  nrow()
#Calculate upper CVR boundary
upper_cvr = 1 + (3*(4+1)/12865)
upper_cvr
#Calculate lower CVR boundary
lower_cvr = 1 - (3*(4+1)/12865)
lower_cvr
#Check for values outside of CVR boundaries
large_res%>%
  filter(covariance.ratios > upper_cvr)%>%
  nrow()
  
large_res%>%
  filter(covariance.ratios < lower_cvr)%>%
  nrow()
```
> After calculating cook's distance, I found no cases problematic since they are not equal to or larger than 1. The covariance ratios were calculated to be between 0.998 and 1.001.

#xii  
Perform the necessary calculations to assess the assumption of independence and state if the condition is met or not.

```{r}
dwt(housing_lm1)
dwt(housing_lm2)
```
> The D-W Statistic for both "housing_lm1" and "housing_lm2" are values that are less than 1 and not even close to a value of 2. Since the values aren't close to 2 that means the assumption of independence for both models are not met.

#xiii  
Perform the necessary calculations to assess the assumption of no multicollinearity and state if the condition is met or not.

```{r}
#VIF
vif(housing_lm2)
#Tolerance
1/vif(housing_lm2)
#Average VIF
mean(vif(housing_lm2))
```
> The VIF values are below 10. Also, the tolerance statistics are above 0.2. In addition the average VIF value is is not that much greater than 1 so it shouldn't be biased. Using these values we can conclude that there is no collinearity with the data. 

#xiv  
Visually check the assumptions related to the residuals using the plot() and hist() functions. Summarize what each graph is informing you of and if any anomalies are present.

```{r}
plot(housing_lm2)
hist(rstudent(housing_lm2))
```

#xv  
Overall, is this regression model unbiased? If an unbiased regression model, what does this tell us about the sample vs. the entire population model?

> Based on the average VIF value, the model should be unbiased.